# @package _global_
num_epochs: 40
architecture_type: TRANSFORMER
dataset_type: WIKITEXT
optimization:
  optimizer_type: ADAM
data_loader:
  train_batch_size: 20
  clip_grad: 0.25
